apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "vlminference.fullname" . }}
  labels:
    app: {{ include "vlminference.name" . }}
spec:
  replicas: {{ .Values.vlminference.replicaCount | default 1 }}
  selector:
    matchLabels:
      app: {{ .Values.vlminference.name }}
  template:
    metadata:
      labels:
        app: {{ .Values.vlminference.name }}
    spec:
      {{- with .Values.securityContext }}
      securityContext:
        {{- toYaml . | nindent 10 }}
      {{- end }}
      {{- with .Values.nodeAffinity }}
      affinity:
        nodeAffinity:
          {{- toYaml . | nindent 10 }}
      {{- end }}
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.vlminference.image.repository }}:{{ .Values.vlminference.image.tag }}"
          imagePullPolicy: {{ .Values.vlminference.image.pullPolicy }}
          {{- with .Values.containerSecurityContext }}
          securityContext:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          ports:
            - containerPort: {{ .Values.containerPort }}
              name: {{ .Values.containerPortName }}
          startupProbe:
            {{- toYaml .Values.startupProbe | nindent 12 }}
          env:
            - name: VLM_MODEL_NAME
              value: {{ required "Value for `global.vlmName` is required!" .Values.global.vlmName | quote }}
            - name: VLM_COMPRESSION_WEIGHT_FORMAT
              value: {{ .Values.vlminference.env.VLM_COMPRESSION_WEIGHT_FORMAT | quote }}
            - name: VLM_DEVICE
              value: {{ .Values.vlminference.env.VLM_DEVICE | quote }}
            - name: VLM_SEED
              value: {{ .Values.vlminference.env.VLM_SEED | quote }}
            - name: http_proxy
              value: {{ .Values.global.proxy.http_proxy | quote }}
            - name: https_proxy
              value: {{ .Values.global.proxy.https_proxy | quote }}
            - name: no_proxy
              value: "{{ .Values.global.proxy.no_proxy }},localhost,127.0.0.1,audioanalyzer,vlm-inference-microservice,videosummarybackend,videoingestion,minio-server,postgresql,video-summary-nginx,rabbitmq,multimodal-embedding-ms,vdms-dataprep,vdms-vectordb,videosearch,.svc.cluster.local"
            - name: NO_PROXY
              value: "{{ .Values.global.proxy.no_proxy }},localhost,127.0.0.1,audioanalyzer,vlm-inference-microservice,videosummarybackend,videoingestion,minio-server,postgresql,video-summary-nginx,rabbitmq,multimodal-embedding-ms,vdms-dataprep,vdms-vectordb,videosearch,.svc.cluster.local"
            - name: no_proxy_env
              value: "{{ .Values.global.proxy.no_proxy }},localhost,127.0.0.1,audioanalyzer,vlm-inference-microservice,videosummarybackend,videoingestion,minio-server,postgresql,video-summary-nginx,rabbitmq,multimodal-embedding-ms,vdms-dataprep,vdms-vectordb,videosearch,.svc.cluster.local"
            - name: MULTI_FRAME_COUNT
              value: {{ .Values.vlminference.env.MULTI_FRAME_COUNT | quote }}
            - name: VLM_CONCURRENT
              value: {{ .Values.vlminference.env.VLM_CONCURRENT | quote }}
            - name: VLM_HOST_PORT
              value: {{ .Values.vlminference.env.VLM_HOST_PORT | quote }}
          volumeMounts:
          - name: workspace
            mountPath: /app/ov-model
            subPath: models
          - name: workspace
            mountPath: /home/appuser/.cache/huggingface
            subPath: huggingface
          securityContext:
            readOnlyRootFilesystem: false
      volumes:
        - name: workspace
          {{- if .Values.global.usePvc }}
          persistentVolumeClaim:
            claimName: {{ include "vlminference.fullname" . }}-pvc
          {{- else if .Values.global.volumeHostPath }}
          hostPath:
            path: {{ .Values.global.volumeHostPath }}
            type: DirectoryOrCreate
          {{- else }}
          emptyDir: {}
          {{- end }}

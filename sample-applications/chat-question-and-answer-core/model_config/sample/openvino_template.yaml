# ChatQnA Core Application Configuration Template
# Customize the values below to suit your application's requirements.

model_settings:
  # Runtime for model serving.
  # Options: "ollama", "openvino"
  MODEL_RUNTIME: "openvino"

  # Embedding model used for vectorizing input text.
  # Example: "BAAI/bge-small-en-v1.5"
  EMBEDDING_MODEL_ID: "BAAI/bge-small-en-v1.5"

  # Reranker model used to reorder retrieved results based on relevance.
  # Example: "BAAI/bge-reranker-base"
  RERANKER_MODEL_ID: "BAAI/bge-reranker-base"

  # Large Language Model (LLM) used to generate responses.
  # Example: "microsoft/Phi-3.5-mini-instruct"
  LLM_MODEL_ID: "microsoft/Phi-3.5-mini-instruct"

  # Prompt template used by the assistant to generate answers.
  # Ensure placeholders like {context} and {question} are preserved.
  # Optional: Adjust the template to match your model's requirements.
  #           If did not provde, will use the default template defined in the application.
  PROMPT_TEMPLATE: >
    <|system|>
    Use the following pieces of context from retrieved
    dataset to answer the question. Do not make up an answer if there is no
    context provided to help answer it.<|end|>

    <|context|>
    Context:
    ---------
    {context}
    <|end|>

    <|user|>
    ---------
    Question: {question}
    ---------
    <|end|>

    <|assistant|>
    Answer:

  # Optional: Maximum number of tokens in the model's response.
  # Maximum token 1024. Not exceeding this limit to avoid errors.
  MAX_TOKENS: 1024

device_settings:
  # Specify the device for each model. Options: "CPU", "GPU"
  EMBEDDING_DEVICE: "CPU"
  RERANKER_DEVICE: "CPU"
  LLM_DEVICE: "CPU"

# Notes:
# - Ensure all model IDs are valid and accessible.
# - Prompt template must be properly formatted and include required placeholders.
# - Adjust device settings based on your system's hardware capabilities.

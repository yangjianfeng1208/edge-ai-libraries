image:
  registry: "intel/"
  pullPolicy: IfNotPresent
  tags:
    openvinoCPU: "core_1.3.0"
    openvinoGPU: "core_gpu_1.3.0"
    ollama: "core_ollama_1.3.0"

configmap:
  enabled: true  # Set to `false` to use default model config in the application

global:
  http_proxy:
  https_proxy:
  no_proxy:
  model_cache_path: "/tmp/model_cache"
  ui_nodeport:
  pvc:
    size: 60Gi
  keeppvc: false # true  to persist models across multiple deployments

chatqna:
  name: chatqna-core
  service:
    type: ClusterIP
    port: 8888
  readinessProbe:
    httpGet:
      path: /v1/chatqna/health
      port: 8888
    initialDelaySeconds: 30
    periodSeconds: 30

gpu:
  enabled: false
  devices: /dev/dri
  group_add: $(stat -c "%g" /dev/dri/render*)
  key:  #update as per the cluster node label key for GPU assigned by device Plugin

uiService:
  name: chatqna-core-ui
  type: ClusterIP
  port: 8102
